{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Real-vs-Fake-News-Classification.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i0JPJVEgP2t",
        "colab_type": "code",
        "outputId": "b0df6d60-c578-470f-a729-5f40d76be829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "#nltk.download('stopwords')\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4T1cHb1gP3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fixedDataset.to_csv('Cleaned-Data.csv')\n",
        "fixedDataset = pd.read_csv('Bert-Rel.csv', index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avDvmDpmwaF2",
        "colab_type": "code",
        "outputId": "28a95e27-e648-4139-8e8a-4eb6bbec154c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "fixedDataset.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>as us budget fight looms republicans flip thei...</td>\n",
              "      <td>washington reuters  the head of a conservative...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>us military to accept transgender recruits on ...</td>\n",
              "      <td>washington reuters  transgender people will be...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>senior us republican senator let mr mueller do...</td>\n",
              "      <td>washington reuters  the special counsel invest...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fbi russia probe helped by australian diplomat...</td>\n",
              "      <td>washington reuters  trump campaign adviser geo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>trump wants postal service to charge much more...</td>\n",
              "      <td>seattlewashington reuters  president donald tr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... Label\n",
              "0  as us budget fight looms republicans flip thei...  ...     0\n",
              "1  us military to accept transgender recruits on ...  ...     0\n",
              "2  senior us republican senator let mr mueller do...  ...     0\n",
              "3  fbi russia probe helped by australian diplomat...  ...     0\n",
              "4  trump wants postal service to charge much more...  ...     0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgchlyAbgP3l",
        "colab_type": "code",
        "outputId": "ccf387b8-a944-4202-8b68-7c59ddf7dd7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "relevant_data = fixedDataset\n",
        "relevant_data.sample(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>41118</th>\n",
              "      <td>threats to business to remove trump sign promp...</td>\n",
              "      <td>please go to the best mulch facebook page and ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42528</th>\n",
              "      <td>austrian parents and teachers sacrifice young ...</td>\n",
              "      <td>anyone up for a european vacation how about se...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23344</th>\n",
              "      <td>report trump is costing america billions in t...</td>\n",
              "      <td>donald trump won t tell you this on his twitte...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25483</th>\n",
              "      <td>donald trump is going to hate what the ny tim...</td>\n",
              "      <td>if you needed more proof that donald trump is ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19438</th>\n",
              "      <td>eu officials reach draft deal on more north ko...</td>\n",
              "      <td>brussels reuters  eu ambassadors have reached ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10111</th>\n",
              "      <td>trump in unexpected meeting with republican pa...</td>\n",
              "      <td>washington reuters  us republican presidential...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3475</th>\n",
              "      <td>any us withdrawal from paris deal no good sign...</td>\n",
              "      <td>berlin reuters  a decision by us president don...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27113</th>\n",
              "      <td>fox news attacks lgbt community with disgusti...</td>\n",
              "      <td>lgbt crowd eyes preschool children  that s th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31484</th>\n",
              "      <td>phoenixrally rocks watch president trump calls...</td>\n",
              "      <td>president trump spoke to a huge and enthusiast...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31028</th>\n",
              "      <td>rino senator flake trashes trump and republica...</td>\n",
              "      <td>the swamp is draining one more rino is not run...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  ... Label\n",
              "41118  threats to business to remove trump sign promp...  ...     1\n",
              "42528  austrian parents and teachers sacrifice young ...  ...     1\n",
              "23344   report trump is costing america billions in t...  ...     1\n",
              "25483   donald trump is going to hate what the ny tim...  ...     1\n",
              "19438  eu officials reach draft deal on more north ko...  ...     0\n",
              "10111  trump in unexpected meeting with republican pa...  ...     0\n",
              "3475   any us withdrawal from paris deal no good sign...  ...     0\n",
              "27113   fox news attacks lgbt community with disgusti...  ...     1\n",
              "31484  phoenixrally rocks watch president trump calls...  ...     1\n",
              "31028  rino senator flake trashes trump and republica...  ...     1\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxCC6UnUgP3s",
        "colab_type": "code",
        "outputId": "8991e454-32d1-4216-f396-66e227f57d6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from collections import Counter\n",
        "\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "fake = Counter()\n",
        "real = Counter()\n",
        "for index, row in relevant_data.iterrows():\n",
        "    # if index == 0:\n",
        "    #     print(row[\"text\"])\n",
        "    #     break\n",
        "    if row[\"Label\"] == 1:\n",
        "        for word in row[\"text\"].split(' '):\n",
        "            if word not in stops and word.isalpha():\n",
        "                fake[word] += 1\n",
        "    else:\n",
        "        for word in row[\"text\"].split(' '):\n",
        "            if word not in stops and word.isalpha():\n",
        "                real[word] += 1\n",
        "        \n",
        "print(real.most_common(5))\n",
        "print(fake.most_common(5))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('said', 98985), ('trump', 42577), ('us', 41137), ('would', 31514), ('reuters', 28306)]\n",
            "[('trump', 73933), ('said', 31013), ('people', 25963), ('president', 25586), ('would', 23427)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5InZdAEa2TyL",
        "colab_type": "code",
        "outputId": "42658578-0977-4b88-fb50-2438275c959e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "plt.bar(real.most_common(5))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-77a3adb8c937>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: bar() argument after ** must be a mapping, not list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1ZQEouxgP3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(real)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYDzgUnMgP32",
        "colab_type": "code",
        "outputId": "6c8ee632-a283-46d7-a33a-c817b2220676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "train_indices = np.random.rand(len(relevant_data)) < 0.8\n",
        "train = relevant_data[train_indices].reset_index(drop=True)\n",
        "test = relevant_data[~train_indices].reset_index(drop=True)\n",
        "print(train.head())\n",
        "print(len(train))\n",
        "print(test.head())\n",
        "print(len(test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                               title  ... Label\n",
            "0  as us budget fight looms republicans flip thei...  ...     0\n",
            "1  us military to accept transgender recruits on ...  ...     0\n",
            "2  senior us republican senator let mr mueller do...  ...     0\n",
            "3  fbi russia probe helped by australian diplomat...  ...     0\n",
            "4  trump wants postal service to charge much more...  ...     0\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "35961\n",
            "                                               title  ... Label\n",
            "0  factbox trump on twitter dec   approval rating...  ...     0\n",
            "1              trump on twitter dec   global warming  ...     0\n",
            "2            trump on twitter dec   trump iraq syria  ...     0\n",
            "3  trump on twitter dec   hillary clinton tax cut...  ...     0\n",
            "4  treasury secretary mnuchin was sent giftwrappe...  ...     0\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "8937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZwXzcRsgP37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train model\n",
        "def training_step(training_data, vectorizer):\n",
        "    training_text = []\n",
        "    training_result = []\n",
        "    for i in range(len(training_data)):\n",
        "        #training_text.append(training_data['text'][i])\n",
        "        training_text.append(' '.join(str(x) for x in training_data['text'][i]))\n",
        "        training_result.append(training_data['Label'][i])\n",
        "    training_text = vectorizer.fit_transform(training_text)   \n",
        "    classifier = MultinomialNB().fit(training_text, training_result) #remove toarray for bernoulli/multi\n",
        "#     print(\"Classifier: \" + str(classifier))\n",
        "#     print(\"Classifier log prior: \" + str(classifier.class_log_prior_))\n",
        "#     print(\"Feature log prob: \" + str(classifier.feature_log_prob_))\n",
        "    classifier.get_params()\n",
        "    return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hh2NCfBgP4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#return block of text and its classifier prediction\n",
        "def analyze_text(classifier, vectorizer, text):\n",
        "#     print(\"Classifier prediction: \" + str(classifier.predict(vectorizer.transform([text]))))\n",
        "    return text, classifier.predict(vectorizer.transform([text]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Mz7NK-gP4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#evaluate model performance\n",
        "def evaluate(evaluation_data, vectorizer):\n",
        "    evaluation_text = []\n",
        "    evaluation_result = []\n",
        "    #separate features/labels\n",
        "    for i in range(len(evaluation_data)):\n",
        "        #training_text.append(training_data['text'][i])\n",
        "        evaluation_text.append(' '.join(str(x) for x in evaluation_data['text'][i]))\n",
        "        evaluation_result.append(evaluation_data['Label'][i])\n",
        "    #check accuracy of classifier on evaluation data\n",
        "    total = len(evaluation_text)\n",
        "    corrects = 0\n",
        "    for i in range(0, total):\n",
        "        result = classifier.predict(vectorizer.transform([evaluation_text[i]])) #remove toarray\n",
        "        text = evaluation_text[i]\n",
        "        corrects += 1 if result[0] == evaluation_result[i] else 0\n",
        "    print(\"Accuracy: \", corrects * 100 / total)\n",
        "    return corrects * 100 / total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL2xhhJpgP4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# v = CountVectorizer('binary=true')\n",
        "v = CountVectorizer()\n",
        "classifier = training_step(train, v)\n",
        "acc = evaluate(test, v)\n",
        "print(acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sM39bvZgP4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mpDiN7-h5DA",
        "colab_type": "code",
        "outputId": "33bd5108-c403-4fb2-827a-c88a833ce62a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 12.8MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 51.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 53.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 54.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.16.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=3e61cdbbb0a67512c832bad250991c7eec483874f013ccdfb53fbe6a7e0da9ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JamyZkJQgP4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETtiBugdgP4a",
        "colab_type": "code",
        "outputId": "07673827-1710-4c5e-8f15-7b60fb65a979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "training_data = train[['title','Label']]\n",
        "training_data.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>as us budget fight looms republicans flip thei...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>us military to accept transgender recruits on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>senior us republican senator let mr mueller do...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fbi russia probe helped by australian diplomat...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>trump wants postal service to charge much more...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>white house congress prepare for talks on spen...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>trump says russia probe will be fair but timel...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>alabama official to certify senatorelect jones...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>jones certified us senate winner despite moore...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>new york governor questions the constitutional...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  Label\n",
              "0  as us budget fight looms republicans flip thei...      0\n",
              "1  us military to accept transgender recruits on ...      0\n",
              "2  senior us republican senator let mr mueller do...      0\n",
              "3  fbi russia probe helped by australian diplomat...      0\n",
              "4  trump wants postal service to charge much more...      0\n",
              "5  white house congress prepare for talks on spen...      0\n",
              "6  trump says russia probe will be fair but timel...      0\n",
              "7  alabama official to certify senatorelect jones...      0\n",
              "8  jones certified us senate winner despite moore...      0\n",
              "9  new york governor questions the constitutional...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1xWzfvwgP4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbSYMmaEgP4i",
        "colab_type": "code",
        "outputId": "5d39002a-4175-4c8e-9a8a-ca6db4efb373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# test tokenizer\n",
        "tokenizer.tokenize(training_data.title[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['as',\n",
              " 'us',\n",
              " 'budget',\n",
              " 'fight',\n",
              " 'lo',\n",
              " '##oms',\n",
              " 'republicans',\n",
              " 'flip',\n",
              " 'their',\n",
              " 'fiscal',\n",
              " 'script']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwi27QcvgP4m",
        "colab_type": "code",
        "outputId": "b512de52-cb57-434f-cb10-e914760725fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# calculate max length of title\n",
        "max_len = 0\n",
        "for t in training_data.title.values:\n",
        "    input_ids = tokenizer.encode(t, add_special_tokens=True)\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "print(max_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhyj3osPgP4q",
        "colab_type": "code",
        "outputId": "a11a877b-231f-4f0c-851f-ec44c63e17fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "input_ids = []\n",
        "at_masks = []\n",
        "for t in training_data.title.values:\n",
        "    encoded = tokenizer.encode_plus(t, add_special_tokens = True, max_length=64, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')\n",
        "    input_ids.append(encoded['input_ids'])\n",
        "    at_masks.append(encoded['attention_mask'])\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "at_masks = torch.cat(at_masks, dim=0)\n",
        "labels = torch.tensor(training_data.Label.values)\n",
        "print(input_ids[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  101,  2004,  2149,  5166,  2954,  8840, 22225, 10643, 11238,  2037,\n",
            "        10807,  5896,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgoxPl9ugP4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split, DataLoader, SequentialSampler, RandomSampler\n",
        "data = TensorDataset(input_ids, at_masks, labels)\n",
        "train_size = int(.9 * len(data))\n",
        "val_size = len(data) - train_size\n",
        "train_data, val_data = random_split(data, [train_size, val_size])\n",
        "train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size = 32)\n",
        "val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size = 32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxH6kcl3k5P9",
        "colab_type": "code",
        "outputId": "b6a4c31f-b593-4de3-95ee-14ffe819b513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2, output_attentions=False, output_hidden_states = False)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUKDEBAHl9s1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 2e-5,eps = 1e-8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_iogbLCnxtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyhX83K1n1th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reccomended in BERT paper\n",
        "epochs = 2\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ZDA6TXoDEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(preds, labels):\n",
        "    preds = np.argmax(preds, axis=1).flatten()\n",
        "    labels = labels.flatten()\n",
        "    return np.sum(preds == labels) / len(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uteYhLGpdzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeTPlFJVpJKA",
        "colab_type": "code",
        "outputId": "59ff5e49-c3e2-4d19-bd8f-17a7dee63601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        }
      },
      "source": [
        "stats = []\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "    print(f'epoch: {epoch}')\n",
        "    start_of_epoch = time.time()\n",
        "    train_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - start_of_epoch)\n",
        "            print(f'Batch {step}  of  {len(train_dataloader)}. Elapsed: {elapsed}.')\n",
        "        batch_input_ids = batch[0].to('cuda')\n",
        "        batch_input_masks = batch[1].to('cuda')\n",
        "        batch_labels = batch[2].to('cuda')\n",
        "        model.zero_grad()\n",
        "        loss, logits = model(batch_input_ids, token_type_ids=None, attention_mask=batch_input_masks, labels=batch_labels)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    training_time = format_time(time.time() - start_of_epoch)\n",
        "    print(f\"average training loss: {avg_train_loss}\")\n",
        "    print(f\"training epcoch took: {training_time}\")\n",
        "    model.eval()\n",
        "\n",
        "    eval_accuracy = 0\n",
        "    eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "\n",
        "        batch_input_ids = batch[0].to('cuda')\n",
        "        batch_input_mask = batch[1].to('cuda')\n",
        "        batch_labels = batch[2].to('cuda')\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "\n",
        "            (loss, logits) = model(batch_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=batch_input_mask,\n",
        "                                   labels=batch_labels)\n",
        "            \n",
        "        eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = batch_labels.to('cpu').numpy()\n",
        "\n",
        "        eval_accuracy += accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    avg_val_accuracy = eval_accuracy / len(val_dataloader)\n",
        "    print(f\"accuracy: {avg_val_accuracy}\")\n",
        "\n",
        "    avg_val_loss = eval_loss / len(val_dataloader)\n",
        "\n",
        "    print(f\"Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "    stats.append(\n",
        "        {\n",
        "            'epoch': epoch + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0\n",
            "  Batch    50  of  1,012.    Elapsed: 0:00:20.\n",
            "  Batch   100  of  1,012.    Elapsed: 0:00:40.\n",
            "  Batch   150  of  1,012.    Elapsed: 0:01:01.\n",
            "  Batch   200  of  1,012.    Elapsed: 0:01:21.\n",
            "  Batch   250  of  1,012.    Elapsed: 0:01:42.\n",
            "  Batch   300  of  1,012.    Elapsed: 0:02:02.\n",
            "  Batch   350  of  1,012.    Elapsed: 0:02:23.\n",
            "  Batch   400  of  1,012.    Elapsed: 0:02:44.\n",
            "  Batch   450  of  1,012.    Elapsed: 0:03:05.\n",
            "  Batch   500  of  1,012.    Elapsed: 0:03:26.\n",
            "  Batch   550  of  1,012.    Elapsed: 0:03:46.\n",
            "  Batch   600  of  1,012.    Elapsed: 0:04:07.\n",
            "  Batch   650  of  1,012.    Elapsed: 0:04:28.\n",
            "  Batch   700  of  1,012.    Elapsed: 0:04:49.\n",
            "  Batch   750  of  1,012.    Elapsed: 0:05:10.\n",
            "  Batch   800  of  1,012.    Elapsed: 0:05:31.\n",
            "  Batch   850  of  1,012.    Elapsed: 0:05:52.\n",
            "  Batch   900  of  1,012.    Elapsed: 0:06:12.\n",
            "  Batch   950  of  1,012.    Elapsed: 0:06:33.\n",
            "  Batch 1,000  of  1,012.    Elapsed: 0:06:54.\n",
            "average training loss: 0.11859246630869481\n",
            "raining epcoh took: 0:06:59\n",
            "accuracy: 0.9753871681415929\n",
            "  Validation Loss: 0.07\n",
            "epoch: 1\n",
            "  Batch    50  of  1,012.    Elapsed: 0:00:21.\n",
            "  Batch   100  of  1,012.    Elapsed: 0:00:42.\n",
            "  Batch   150  of  1,012.    Elapsed: 0:01:02.\n",
            "  Batch   200  of  1,012.    Elapsed: 0:01:23.\n",
            "  Batch   250  of  1,012.    Elapsed: 0:01:44.\n",
            "  Batch   300  of  1,012.    Elapsed: 0:02:05.\n",
            "  Batch   350  of  1,012.    Elapsed: 0:02:26.\n",
            "  Batch   400  of  1,012.    Elapsed: 0:02:47.\n",
            "  Batch   450  of  1,012.    Elapsed: 0:03:08.\n",
            "  Batch   500  of  1,012.    Elapsed: 0:03:29.\n",
            "  Batch   550  of  1,012.    Elapsed: 0:03:50.\n",
            "  Batch   600  of  1,012.    Elapsed: 0:04:10.\n",
            "  Batch   650  of  1,012.    Elapsed: 0:04:31.\n",
            "  Batch   700  of  1,012.    Elapsed: 0:04:52.\n",
            "  Batch   750  of  1,012.    Elapsed: 0:05:13.\n",
            "  Batch   800  of  1,012.    Elapsed: 0:05:34.\n",
            "  Batch   850  of  1,012.    Elapsed: 0:05:55.\n",
            "  Batch   900  of  1,012.    Elapsed: 0:06:16.\n",
            "  Batch   950  of  1,012.    Elapsed: 0:06:36.\n",
            "  Batch 1,000  of  1,012.    Elapsed: 0:06:57.\n",
            "average training loss: 0.03620813093921207\n",
            "raining epcoh took: 0:07:02\n",
            "accuracy: 0.9787057522123894\n",
            "  Validation Loss: 0.07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoX6dcniO1_a",
        "colab_type": "code",
        "outputId": "a1ba9e4c-46aa-48f6-b658-2f9eb41d361a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "\n",
        "df_stats = pd.DataFrame(data=stats)\n",
        "\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "df_stats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.12</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0:06:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.04</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0:07:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time\n",
              "epoch                                                         \n",
              "1               0.12         0.07           0.98       0:06:59\n",
              "2               0.04         0.07           0.98       0:07:02"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hte0-CFspw6o",
        "colab_type": "code",
        "outputId": "d18b83a9-b51c-4226-820d-6b8b0421b2e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2])\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ-Ofy-0O6bZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = []\n",
        "at_masks = []\n",
        "\n",
        "for sent in test.title.values:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = 64,           \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True, \n",
        "                        return_tensors = 'pt',\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    at_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "at_masks = torch.cat(at_masks, dim=0)\n",
        "labels = torch.tensor(test.Label.values)\n",
        "\n",
        "batch_size = 32  \n",
        "\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShTpYKKEWKY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model.eval()\n",
        "\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to('cuda') for t in batch)\n",
        "  \n",
        "  batch_input_ids, batch_input_mask, batch_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(batch_input_ids, token_type_ids=None, attention_mask=batch_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = batch_labels.to('cpu').numpy()\n",
        "\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqRsbIkVW7dB",
        "colab_type": "code",
        "outputId": "467f1c6f-a3f1-4fa7-e41e-9a0534dbb392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predlabels = []\n",
        "for i in range(len(true_labels)):\n",
        "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "    predlabels.append(pred_labels_i)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JYsP3igcRAi",
        "colab_type": "code",
        "outputId": "56acffdb-db14-4121-c532-3a7e317ec653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(predlabels[10])\n",
        "len(predlabels[0])\n",
        "print(true_labels[10])\n",
        "print(len(true_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVKGe5ORXNKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_accuracy = 0\n",
        "for i in range(len(true_labels)):\n",
        "    test_accuracy += np.sum(np.array(predlabels[i]) == np.array(true_labels[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeJzKluIbWQI",
        "colab_type": "code",
        "outputId": "fed5622d-1ffe-4633-bd12-6e80331ebb8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_accuracy / len(true_labels*32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9736607142857143"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2pbR2VReYaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}